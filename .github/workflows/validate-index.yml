name: Validate Index

on:
  pull_request_target:
    branches: [main]
    paths:
      - 'index.json'

permissions:
  pull-requests: write
  contents: write

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          fetch-depth: 0

      - name: Validate index.json schema
        id: schema
        run: |
          python3 -c "
          import json, re, sys

          with open('index.json') as f:
              data = json.load(f)

          errors = []

          # Check version
          if data.get('version') != 1:
              errors.append(f'version must be 1, got {data.get(\"version\")}')

          packs = data.get('packs', [])
          if not isinstance(packs, list):
              errors.append('packs must be an array')
              print('\n'.join(errors))
              sys.exit(1)

          REQUIRED_FIELDS = [
              'name', 'display_name', 'version', 'trust_tier',
              'categories', 'language', 'sound_count',
              'source_repo', 'source_ref', 'source_path',
          ]

          NAME_RE = re.compile(r'^[a-z0-9][a-z0-9_-]{0,63}$')
          VERSION_RE = re.compile(r'^\d+\.\d+\.\d+$')
          VALID_TIERS = {'official', 'verified', 'community'}

          seen_names = set()
          prev_name = ''

          for i, pack in enumerate(packs):
              prefix = f'packs[{i}]'

              # Required fields
              for field in REQUIRED_FIELDS:
                  if field not in pack:
                      errors.append(f'{prefix}: missing required field \"{field}\"')

              name = pack.get('name', '')

              # Name pattern
              if name and not NAME_RE.match(name):
                  errors.append(f'{prefix}: name \"{name}\" does not match ^[a-z0-9][a-z0-9_-]{{0,63}}$')

              # Name uniqueness
              if name in seen_names:
                  errors.append(f'{prefix}: duplicate name \"{name}\"')
              seen_names.add(name)

              # Alphabetical sort order
              if name and name < prev_name:
                  errors.append(f'{prefix}: name \"{name}\" is out of alphabetical order (after \"{prev_name}\")')
              prev_name = name

              # Version pattern
              version = pack.get('version', '')
              if version and not VERSION_RE.match(version):
                  errors.append(f'{prefix}: version \"{version}\" does not match ^\\d+\\.\\d+\\.\\d+$')

              # Trust tier
              tier = pack.get('trust_tier', '')
              if tier and tier not in VALID_TIERS:
                  errors.append(f'{prefix}: trust_tier \"{tier}\" not in {VALID_TIERS}')

          if errors:
              print(f'Validation failed with {len(errors)} error(s):')
              for e in errors:
                  print(f'  - {e}')
              # Write errors for review comment
              with open('schema_errors.json', 'w') as f:
                  json.dump(errors, f)
              sys.exit(1)

          print(f'index.json is valid: {len(packs)} packs')
          with open('schema_errors.json', 'w') as f:
              json.dump([], f)
          "

      - name: Detect new/changed packs
        id: detect
        if: always()
        run: |
          python3 -c "
          import json, sys

          # Load PR version
          with open('index.json') as f:
              pr_data = json.load(f)
          pr_packs = {p['name']: p for p in pr_data.get('packs', [])}

          # Load base version (main branch)
          import subprocess
          result = subprocess.run(['git', 'show', 'origin/main:index.json'], capture_output=True, text=True)
          if result.returncode == 0:
              base_data = json.loads(result.stdout)
              base_packs = {p['name']: p for p in base_data.get('packs', [])}
          else:
              base_packs = {}

          # Find new or changed packs
          new_packs = []
          updated_packs = []
          for name, pack in pr_packs.items():
              if name not in base_packs:
                  new_packs.append(name)
              elif pack != base_packs[name]:
                  updated_packs.append(name)

          changed = new_packs + updated_packs

          # Write detection results
          results = {
              'new': new_packs,
              'updated': updated_packs,
              'packs': {n: pr_packs[n] for n in changed}
          }
          with open('detect_results.json', 'w') as f:
              json.dump(results, f)

          if changed:
              names = ', '.join(changed)
              print(f'Changed packs: {names}')
              with open('changed_packs.json', 'w') as f:
                  json.dump([pr_packs[n] for n in changed], f)
          else:
              print('No pack changes detected')
              with open('changed_packs.json', 'w') as f:
                  json.dump([], f)
          "

      - name: Validate trust tier
        id: trust
        if: always() && steps.detect.outcome == 'success'
        env:
          PR_AUTHOR_ASSOCIATION: ${{ github.event.pull_request.author_association }}
        run: |
          python3 << 'PYEOF'
          import json, os, sys

          association = os.environ.get('PR_AUTHOR_ASSOCIATION', '')
          is_org_member = association in ('OWNER', 'MEMBER')

          with open('changed_packs.json') as f:
              packs = json.load(f)

          errors = []
          for pack in packs:
              tier = pack.get('trust_tier', '')
              name = pack.get('name', '')
              if not is_org_member and tier in ('official', 'verified'):
                  errors.append(f'{name}: community contributors cannot use trust_tier "{tier}" (use "community")')

          with open('trust_errors.json', 'w') as f:
              json.dump(errors, f)

          if errors:
              for e in errors:
                  print(f'Error: {e}')
              sys.exit(1)
          else:
              print(f'Trust tier check passed (author association: {association})')
          PYEOF

      - name: Validate pack sources
        id: sources
        if: always() && steps.detect.outcome == 'success'
        run: |
          python3 << 'PYEOF'
          import json, urllib.request, hashlib, sys, os

          with open('changed_packs.json') as f:
              packs = json.load(f)

          if not packs:
              print("No new/changed packs to validate")
              with open('source_results.json', 'w') as f:
                  json.dump({'errors': [], 'warnings': [], 'checks': {}}, f)
              sys.exit(0)

          ALLOWED_EXTENSIONS = {'.wav', '.mp3', '.ogg', '.flac', '.m4a'}
          MAX_PACK_SIZE = 50 * 1024 * 1024  # 50MB max per pack
          MAX_FILE_SIZE = 10 * 1024 * 1024   # 10MB max per file
          errors = []
          warnings = []
          checks = {}  # per-pack check results

          for pack in packs:
              name = pack['name']
              repo = pack.get('source_repo', '')
              ref = pack.get('source_ref', 'main')
              path = pack.get('source_path', '.')
              pack_checks = {'manifest': False, 'sha256': None, 'audio_files': []}
              print(f"\n=== Validating {name} ({repo}@{ref}) ===")

              if not repo:
                  errors.append(f"{name}: missing source_repo")
                  checks[name] = pack_checks
                  continue

              # Build base URL
              if path and path != '.':
                  base = f"https://raw.githubusercontent.com/{repo}/{ref}/{path}"
              else:
                  base = f"https://raw.githubusercontent.com/{repo}/{ref}"

              # 1. Fetch and validate manifest
              manifest_url = f"{base}/openpeon.json"
              try:
                  req = urllib.request.urlopen(manifest_url, timeout=15)
                  manifest_bytes = req.read()
                  manifest = json.loads(manifest_bytes)
                  pack_checks['manifest'] = True
                  print(f"  Manifest: OK ({len(manifest_bytes)} bytes)")
              except Exception as e:
                  errors.append(f"{name}: cannot fetch manifest at {manifest_url}: {e}")
                  checks[name] = pack_checks
                  continue

              # 2. Verify manifest SHA256 if provided
              claimed_sha = pack.get('manifest_sha256', '')
              if claimed_sha:
                  actual_sha = hashlib.sha256(manifest_bytes).hexdigest()
                  if actual_sha != claimed_sha:
                      errors.append(f"{name}: manifest SHA256 mismatch (claimed {claimed_sha[:16]}... got {actual_sha[:16]}...)")
                      pack_checks['sha256'] = False
                  else:
                      pack_checks['sha256'] = True
                      print(f"  SHA256: OK")
              else:
                  pack_checks['sha256'] = None  # not provided

              # 3. Validate manifest structure
              if 'name' not in manifest:
                  errors.append(f"{name}: manifest missing 'name' field")
              if 'categories' not in manifest:
                  errors.append(f"{name}: manifest missing 'categories' field")
                  checks[name] = pack_checks
                  continue

              # 4. Collect all sound files
              sound_files = set()
              for cat_name, cat_data in manifest.get('categories', {}).items():
                  for sound in cat_data.get('sounds', []):
                      f = sound.get('file', '')
                      if f:
                          sound_files.add(os.path.basename(f))

              if not sound_files:
                  errors.append(f"{name}: no sound files referenced in manifest")
                  checks[name] = pack_checks
                  continue

              claimed_count = pack.get('sound_count', 0)
              if claimed_count and len(sound_files) != claimed_count:
                  warnings.append(f"{name}: sound_count={claimed_count} but manifest references {len(sound_files)} unique files")

              print(f"  Sound files: {len(sound_files)} referenced")

              # 5. Spot-check up to 3 sound files exist and are audio
              for sfile in sorted(sound_files)[:3]:
                  sound_url = f"{base}/sounds/{sfile}"
                  try:
                      req = urllib.request.urlopen(sound_url, timeout=15)
                      data = req.read(256)
                      size = int(req.headers.get('Content-Length', 0))

                      ext = os.path.splitext(sfile)[1].lower()
                      if ext not in ALLOWED_EXTENSIONS:
                          errors.append(f"{name}: {sfile} has disallowed extension '{ext}'")
                          pack_checks['audio_files'].append({'file': sfile, 'ok': False})
                          continue

                      is_audio = False
                      if data[:4] == b'RIFF':
                          is_audio = True
                      elif data[:3] == b'ID3' or data[:2] == b'\xff\xfb' or data[:2] == b'\xff\xf3':
                          is_audio = True
                      elif data[:4] == b'OggS':
                          is_audio = True
                      elif data[:4] == b'fLaC':
                          is_audio = True
                      elif data[4:8] == b'ftyp':
                          is_audio = True

                      if not is_audio:
                          errors.append(f"{name}: {sfile} does not appear to be a valid audio file")
                          pack_checks['audio_files'].append({'file': sfile, 'ok': False})
                      else:
                          pack_checks['audio_files'].append({'file': sfile, 'ok': True, 'size': size})
                          print(f"  {sfile}: OK ({ext}, {size} bytes)")

                      if size > MAX_FILE_SIZE:
                          errors.append(f"{name}: {sfile} is {size / 1024 / 1024:.1f}MB (max {MAX_FILE_SIZE / 1024 / 1024:.0f}MB)")

                  except Exception as e:
                      errors.append(f"{name}: cannot fetch sound {sfile}: {e}")
                      pack_checks['audio_files'].append({'file': sfile, 'ok': False})

              # 6. Collect icon files from manifest
              MAX_ICON_SIZE = 500 * 1024  # 500KB max per icon
              icon_files = set()
              pack_icon = manifest.get('icon', '')
              if pack_icon:
                  icon_files.add(pack_icon)
              for cat_name, cat_data in manifest.get('categories', {}).items():
                  cat_icon = cat_data.get('icon', '')
                  if cat_icon:
                      icon_files.add(cat_icon)
                  for sound in cat_data.get('sounds', []):
                      sound_icon = sound.get('icon', '')
                      if sound_icon:
                          icon_files.add(sound_icon)

              # 7. Spot-check up to 2 icon files
              if icon_files:
                  print(f"  Icon files: {len(icon_files)} referenced")
                  pack_checks['icon_files'] = []
                  for icon_path in sorted(icon_files)[:2]:
                      icon_url = f"{base}/{icon_path}"
                      try:
                          req = urllib.request.urlopen(icon_url, timeout=15)
                          data = req.read(512)
                          size = int(req.headers.get('Content-Length', len(data)))

                          is_image = False
                          ext = os.path.splitext(icon_path)[1].lower()
                          if data[:4] == b'\x89PNG':
                              is_image = True
                          elif data[:3] == b'\xff\xd8\xff':
                              is_image = True
                          elif data[:4] == b'RIFF' and data[8:12] == b'WEBP':
                              is_image = True
                          elif b'<svg' in data[:256]:
                              is_image = True

                          if not is_image:
                              warnings.append(f"{name}: icon {icon_path} does not appear to be a valid image file")
                              pack_checks['icon_files'].append({'file': icon_path, 'ok': False})
                          elif size > MAX_ICON_SIZE:
                              warnings.append(f"{name}: icon {icon_path} is {size / 1024:.0f}KB (max 500KB)")
                              pack_checks['icon_files'].append({'file': icon_path, 'ok': False, 'size': size})
                          else:
                              pack_checks['icon_files'].append({'file': icon_path, 'ok': True, 'size': size})
                              print(f"  {icon_path}: OK (icon, {size} bytes)")

                      except Exception as e:
                          warnings.append(f"{name}: cannot fetch icon {icon_path}: {e}")
                          pack_checks['icon_files'].append({'file': icon_path, 'ok': False})

              # 8. Check claimed total size
              claimed_size = pack.get('total_size_bytes', 0)
              if claimed_size and claimed_size > MAX_PACK_SIZE:
                  errors.append(f"{name}: total_size_bytes={claimed_size} exceeds {MAX_PACK_SIZE / 1024 / 1024:.0f}MB limit")

              checks[name] = pack_checks

          # Save results
          with open('source_results.json', 'w') as f:
              json.dump({'errors': errors, 'warnings': warnings, 'checks': checks}, f)

          # Report
          print("\n" + "=" * 60)
          if warnings:
              print(f"\nWarnings ({len(warnings)}):")
              for w in warnings:
                  print(f"  - {w}")

          if errors:
              print(f"\nErrors ({len(errors)}):")
              for e in errors:
                  print(f"  - {e}")
              sys.exit(1)
          else:
              print(f"\nAll {len(packs)} pack(s) validated successfully")
          PYEOF

      - name: Write verdict
        if: always()
        env:
          SCHEMA_RESULT: ${{ steps.schema.outcome }}
          TRUST_RESULT: ${{ steps.trust.outcome }}
          SOURCES_RESULT: ${{ steps.sources.outcome }}
        run: |
          if [ "$SCHEMA_RESULT" = "success" ] && [ "$TRUST_RESULT" = "success" ] && [ "$SOURCES_RESULT" = "success" ]; then
            echo "pass" > verdict.txt
          else
            echo "fail" > verdict.txt
          fi
          echo "Verdict: $(cat verdict.txt)"

      - name: Post review comment
        if: always()
        continue-on-error: true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SCHEMA_RESULT: ${{ steps.schema.outcome }}
          TRUST_RESULT: ${{ steps.trust.outcome }}
          SOURCES_RESULT: ${{ steps.sources.outcome }}
        run: |
          python3 << 'PYEOF'
          import json, os, subprocess

          pr = os.environ['PR_NUMBER']
          schema_ok = os.environ.get('SCHEMA_RESULT') == 'success'
          trust_ok = os.environ.get('TRUST_RESULT') == 'success'
          sources_ok = os.environ.get('SOURCES_RESULT') == 'success'
          all_ok = schema_ok and trust_ok and sources_ok

          # Load detection results
          detect = {}
          try:
              with open('detect_results.json') as f:
                  detect = json.load(f)
          except:
              pass

          new_packs = detect.get('new', [])
          updated_packs = detect.get('updated', [])
          pack_details = detect.get('packs', {})

          # Load source validation results
          source_results = {'errors': [], 'warnings': [], 'checks': {}}
          try:
              with open('source_results.json') as f:
                  source_results = json.load(f)
          except:
              pass

          # Load trust errors
          trust_errors = []
          try:
              with open('trust_errors.json') as f:
                  trust_errors = json.load(f)
          except:
              pass

          # Load schema errors
          schema_errors = []
          try:
              with open('schema_errors.json') as f:
                  schema_errors = json.load(f)
          except:
              pass

          # Build comment
          icon = "white_check_mark" if all_ok else "x"
          lines = [f"## :{icon}: Pack Registry Review\n"]

          # Pack summary
          if new_packs or updated_packs:
              lines.append("### Packs\n")
              for name in new_packs:
                  p = pack_details.get(name, {})
                  display = p.get('display_name', name)
                  tier = p.get('trust_tier', '?')
                  count = p.get('sound_count', '?')
                  lang = p.get('language', '?')
                  lines.append(f"- **{display}** (`{name}`) — NEW | {tier} | {lang} | {count} sounds")
              for name in updated_packs:
                  p = pack_details.get(name, {})
                  display = p.get('display_name', name)
                  lines.append(f"- **{display}** (`{name}`) — UPDATED")
              lines.append("")

          # Checks table
          lines.append("### Checks\n")
          lines.append("| Check | Result |")
          lines.append("|---|---|")
          lines.append(f"| Schema validation | {'pass' if schema_ok else 'FAIL'} |")
          lines.append(f"| Trust tier | {'pass' if trust_ok else 'FAIL'} |")
          lines.append(f"| Manifest fetch | {'pass' if sources_ok else 'FAIL'} |")

          # Per-pack source checks
          checks = source_results.get('checks', {})
          for name, c in checks.items():
              if c.get('sha256') is True:
                  lines.append(f"| SHA256 ({name}) | pass |")
              elif c.get('sha256') is False:
                  lines.append(f"| SHA256 ({name}) | FAIL |")

              audio = c.get('audio_files', [])
              if audio:
                  ok_count = sum(1 for a in audio if a.get('ok'))
                  total = len(audio)
                  status = 'pass' if ok_count == total else 'FAIL'
                  files = ', '.join(a['file'] for a in audio)
                  lines.append(f"| Audio spot-check ({name}) | {status} ({ok_count}/{total}: {files}) |")

          lines.append("")

          # Errors
          all_errors = schema_errors + trust_errors + source_results.get('errors', [])
          if all_errors:
              lines.append("### Errors\n")
              for e in all_errors:
                  lines.append(f"- {e}")
              lines.append("")

          # Warnings
          if source_results.get('warnings'):
              lines.append("### Warnings\n")
              for w in source_results['warnings']:
                  lines.append(f"- {w}")
              lines.append("")

          # Verdict
          if all_ok:
              lines.append("All checks passed. Auto-approving and enabling auto-merge.")
          else:
              lines.append("Some checks failed. Please fix the issues above and push again.")

          comment = '\n'.join(lines)

          # Post comment
          subprocess.run(
              ['gh', 'pr', 'comment', pr, '--body', comment],
              check=True
          )
          print("Posted review comment")
          PYEOF

      - name: Auto-approve
        if: always()
        continue-on-error: true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
        run: |
          VERDICT=$(cat verdict.txt 2>/dev/null || echo "fail")
          if [ "$VERDICT" = "pass" ]; then
            gh pr review "$PR_NUMBER" --approve --body "All automated checks passed."
            echo "PR approved"
          else
            echo "Skipping approval — checks did not pass"
          fi

      - name: Enable auto-merge
        if: always()
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
        run: |
          VERDICT=$(cat verdict.txt 2>/dev/null || echo "fail")
          if [ "$VERDICT" = "pass" ]; then
            gh pr merge "$PR_NUMBER" --auto --squash --delete-branch || echo "Auto-merge could not be enabled (branch protection may not be configured)"
            echo "Auto-merge enabled"
          else
            echo "Skipping auto-merge — checks did not pass"
          fi
